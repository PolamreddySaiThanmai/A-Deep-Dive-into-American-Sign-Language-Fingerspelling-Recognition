{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2efa1144",
   "metadata": {
    "papermill": {
     "duration": 0.011569,
     "end_time": "2023-08-27T08:15:06.653959",
     "exception": false,
     "start_time": "2023-08-27T08:15:06.642390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Install libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f009f41b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:06.679212Z",
     "iopub.status.busy": "2023-08-27T08:15:06.678426Z",
     "iopub.status.idle": "2023-08-27T08:15:19.204539Z",
     "shell.execute_reply": "2023-08-27T08:15:19.203030Z"
    },
    "papermill": {
     "duration": 12.54211,
     "end_time": "2023-08-27T08:15:19.207517",
     "exception": false,
     "start_time": "2023-08-27T08:15:06.665407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from icecream import ic\n",
    "except Exception:\n",
    "  !pip install -q icecream --no-index --find-links=file:///kaggle/input/icecream"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794d65b1",
   "metadata": {
    "papermill": {
     "duration": 0.010799,
     "end_time": "2023-08-27T08:15:19.230197",
     "exception": false,
     "start_time": "2023-08-27T08:15:19.219398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcd6df8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:19.255430Z",
     "iopub.status.busy": "2023-08-27T08:15:19.254518Z",
     "iopub.status.idle": "2023-08-27T08:15:34.970181Z",
     "shell.execute_reply": "2023-08-27T08:15:34.968881Z"
    },
    "papermill": {
     "duration": 15.732382,
     "end_time": "2023-08-27T08:15:34.973689",
     "exception": false,
     "start_time": "2023-08-27T08:15:19.241307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n",
      "ic| tf.__version__: '2.12.0', torch.__version__: '2.0.0+cpu'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('2.12.0', '2.0.0+cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import six\n",
    "import glob\n",
    "import traceback\n",
    "import inspect\n",
    "from typing import Union\n",
    "from collections import Counter, OrderedDict, defaultdict\n",
    "from collections.abc import Iterable\n",
    "from multiprocessing import cpu_count\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from icecream import ic\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "ic(tf.__version__, torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd9a602",
   "metadata": {
    "papermill": {
     "duration": 0.012971,
     "end_time": "2023-08-27T08:15:34.999833",
     "exception": false,
     "start_time": "2023-08-27T08:15:34.986862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Force tf not use gpu memory, for only use tf for tfrecord reading,preprocessing and postprocessing, not for train(using torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e353ea38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.028455Z",
     "iopub.status.busy": "2023-08-27T08:15:35.027627Z",
     "iopub.status.idle": "2023-08-27T08:15:35.041053Z",
     "shell.execute_reply": "2023-08-27T08:15:35.039604Z"
    },
    "papermill": {
     "duration": 0.030622,
     "end_time": "2023-08-27T08:15:35.043473",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.012851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices([], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0744b2",
   "metadata": {
    "papermill": {
     "duration": 0.01265,
     "end_time": "2023-08-27T08:15:35.069286",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.056636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb02e114",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.097515Z",
     "iopub.status.busy": "2023-08-27T08:15:35.097067Z",
     "iopub.status.idle": "2023-08-27T08:15:35.111372Z",
     "shell.execute_reply": "2023-08-27T08:15:35.109985Z"
    },
    "papermill": {
     "duration": 0.031576,
     "end_time": "2023-08-27T08:15:35.113921",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.082345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FLAGS(object):\n",
    "  # for tfrecords args, you could ignore\n",
    "  seed = 1024\n",
    "  batch_parse = False\n",
    "  sparse_to_dense = True\n",
    "  eval_keys = []\n",
    "  incl_keys = []\n",
    "  excl_keys = []\n",
    "  recount_tfrecords = False  \n",
    "  batch_sizes = []\n",
    "  buffer_size = 1024\n",
    "  buckets = None\n",
    "  drop_remainder = None\n",
    "  shard_by_files = True\n",
    "  shuffle_batch = None\n",
    "  shuffle_files = None\n",
    "  num_dataset_threads = 0\n",
    "  num_prefetch_batches = 1024\n",
    "  repeat_then_shuffle = False\n",
    "  length_index = 1\n",
    "  length_key = None\n",
    "  dynamic_pad = True\n",
    "  cache = False\n",
    "  cache_after_map = False\n",
    "  fixed_random = False\n",
    "  parallel_read_files = True\n",
    "  padding_idx = 0\n",
    "  dataset_keys = []\n",
    "  dataset_excl_keys = []\n",
    "  exclude_varlen_keys = False\n",
    "  prefetch = None\n",
    "  dataset_ordered = False\n",
    "    \n",
    "  torch = True\n",
    "  keras = False\n",
    "    \n",
    "  # online==False means using n-fold split and train on fold 1,2, folds-1 while valid on fold 0\n",
    "  # online==True means using all train data but still will valid on fold 0\n",
    "  online = False  \n",
    "  folds = 4\n",
    "  fold = 0\n",
    "  fold_seed = 1229\n",
    "  root = '../input/asl-fingerspelling'\n",
    "  working = '/kaggle/working'\n",
    "  use_z = True  # use x,y,z if True\n",
    "  norm_frames = True # norm frames using x - mean / std\n",
    "  concat_frames = True # concat original and normalized frames\n",
    "  add_pos = True # add abs frame pos, like 1/1000., 2/1000.\n",
    "  sup_weight = 0.1 # for supplement dataset assigin weight 0.1\n",
    "  \n",
    "  train_files = []\n",
    "  valid_files = []\n",
    "      \n",
    "  mix_sup = True # train & sup dataset\n",
    "  vie = 5 # valid interval epochs \n",
    "  lr = 2e-3\n",
    "  epochs = 400 \n",
    "  batch_size = 128\n",
    "  eval_batch_size = 256\n",
    "  awp = True\n",
    "  adv_start_epoch = None\n",
    "  adv_lr = 0.2\n",
    "  adv_eps = 0\n",
    "  fp16 = False # notice fp16 could not be set True if using awp here, otherwise nan\n",
    "  optimizer = 'Adam'\n",
    "  opt_eps = 1e-6 \n",
    "  scheduler = 'linear'\n",
    "  # for model related configs\n",
    "  encoder_layers = 17\n",
    "  encoder_units = 200 \n",
    "  n_frames = 320  \n",
    "  distributed = False\n",
    "\n",
    "  # for preprocess\n",
    "  trunct_method = 'resize'\n",
    "\n",
    "  # for aug\n",
    "  flip_rate = 0.25\n",
    "  resample_rate = 0.8\n",
    "  temporal_mask_rate = 0.8\n",
    "  temporal_mask_prob = 0.5\n",
    "  temporal_mask_range = [0.1, 0.5]\n",
    "  spatio_mask_rate = 0.\n",
    "  spatio_mask_prob = 0.\n",
    "  add_pos_before_resample = True\n",
    "  shift_rate = 0.\n",
    "  shift_range = [-0.05, 0.05]\n",
    "  shift_method = 1\n",
    "  temporal_seq_mask_rate = 0.5\n",
    "  temporal_seq_mask_max = 2\n",
    "  temporal_seq_mask_range = [0.1, 0.2]\n",
    "  shift_rate = 0.75\n",
    "  scale_method = 0\n",
    "  scale_rate = 0.75\n",
    "  rotate_rate = 0.75\n",
    "  \n",
    "  # for model\n",
    "  ksize_vals = [15]\n",
    "  skip_factor = 0.5\n",
    "  mhatt_heads = 8\n",
    "  mhatt_dimhead = 32\n",
    "\n",
    "FLAGS.adv_start_epoch = int(FLAGS.epochs * 0.15)\n",
    "    \n",
    "def load_json(filename):\n",
    "  with open(filename) as fh:\n",
    "    obj = json.load(fh)\n",
    "  return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb32fc3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.146210Z",
     "iopub.status.busy": "2023-08-27T08:15:35.145775Z",
     "iopub.status.idle": "2023-08-27T08:15:35.150886Z",
     "shell.execute_reply": "2023-08-27T08:15:35.149555Z"
    },
    "papermill": {
     "duration": 0.024783,
     "end_time": "2023-08-27T08:15:35.153122",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.128339",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLAGS.batch_size = 128\n",
    "FLAGS.acc_steps = 4 # gradient acc\n",
    "FLAGS.batch_size = FLAGS.batch_size // FLAGS.acc_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd10365",
   "metadata": {
    "papermill": {
     "duration": 0.012723,
     "end_time": "2023-08-27T08:15:35.178864",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.166141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Common configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e67318af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.207333Z",
     "iopub.status.busy": "2023-08-27T08:15:35.206914Z",
     "iopub.status.idle": "2023-08-27T08:15:35.630949Z",
     "shell.execute_reply": "2023-08-27T08:15:35.629534Z"
    },
    "papermill": {
     "duration": 0.452409,
     "end_time": "2023-08-27T08:15:35.644575",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.192166",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| len(LIP): 40\n",
      "ic| len(LLIP): 18, len(RLIP): 18, len(MID_LIP): 4\n",
      "ic| N_COLS: 384\n",
      "ic| N_CHARS: 59\n",
      "ic| PAD_IDX: 0, SOS_IDX: 0, EOS_IDX: 60\n",
      "ic| VOCAB_SIZE: 61\n",
      "ic| len(IDX2CHAR): 61\n"
     ]
    }
   ],
   "source": [
    "LPOSE = [13, 15, 17, 19, 21]\n",
    "RPOSE = [14, 16, 18, 20, 22]\n",
    "POSE = LPOSE + RPOSE\n",
    "\n",
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "ic(len(LIP))\n",
    "LLIP = [84,181,91,146,61,185,40,39,37,87,178,88,95,78,191,80,81,82]\n",
    "RLIP = [314,405,321,375,291,409,270,269,267,317,402,318,324,308,415,310,311,312]\n",
    "MID_LIP = [i for i in LIP if i not in LLIP + RLIP]\n",
    "ic(len(LLIP), len(RLIP), len(MID_LIP))\n",
    "\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "LNOSE = [98]\n",
    "RNOSE = [327]\n",
    "MID_NOSE = [i for i in NOSE if i not in LNOSE + RNOSE]\n",
    "\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "\n",
    "N_HAND_POINTS = 21\n",
    "N_POSE_POINTS = len(LPOSE)\n",
    "N_LIP_POINTS = len(LLIP)\n",
    "N_EYE_POINTS = len(LEYE)\n",
    "N_NOSE_POINTS = len(LNOSE)\n",
    "N_MID_POINTS = len(MID_LIP + MID_NOSE)\n",
    "\n",
    "SEL_COLS = []\n",
    "for i in range(N_HAND_POINTS):\n",
    "  SEL_COLS.extend([f'x_left_hand_{i}', f'y_left_hand_{i}', f'z_left_hand_{i}'])\n",
    "for i in range(N_HAND_POINTS):\n",
    "  SEL_COLS.extend([f'x_right_hand_{i}', f'y_right_hand_{i}', f'z_right_hand_{i}'])\n",
    "for i in LPOSE:\n",
    "  SEL_COLS.extend([f'x_pose_{i}', f'y_pose_{i}', f'z_pose_{i}'])\n",
    "for i in RPOSE:\n",
    "  SEL_COLS.extend([f'x_pose_{i}', f'y_pose_{i}', f'z_pose_{i}'])\n",
    "for i in LLIP:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "for i in RLIP:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "\n",
    "for i in LEYE:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "for i in REYE:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "  \n",
    "for i in LNOSE:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "for i in RNOSE:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "  \n",
    "for i in MID_LIP:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "for i in MID_NOSE:\n",
    "  SEL_COLS.extend([f'x_face_{i}', f'y_face_{i}', f'z_face_{i}'])\n",
    "    \n",
    "N_COLS = len(SEL_COLS)\n",
    "ic(N_COLS)\n",
    "    \n",
    "CHAR2IDX = load_json(f'../input/asl-fingerspelling/character_to_prediction_index.json')\n",
    "CHAR2IDX = {k: v + 1 for k, v in CHAR2IDX.items()}\n",
    "N_CHARS = len(CHAR2IDX)\n",
    "ic(N_CHARS)\n",
    "\n",
    "PAD_IDX = 0\n",
    "SOS_IDX = PAD_IDX # Start Of Sentence\n",
    "EOS_IDX = N_CHARS + 1 # End Of Sentence\n",
    "ic(PAD_IDX, SOS_IDX, EOS_IDX)\n",
    "\n",
    "PAD_TOKEN = '<PAD>'\n",
    "SOS_TOKEN = PAD_TOKEN\n",
    "EOS_TOKEN = '<EOS>'\n",
    "\n",
    "CHAR2IDX[PAD_TOKEN] = PAD_IDX\n",
    "CHAR2IDX[EOS_TOKEN] = EOS_IDX \n",
    "\n",
    "ADDRESS_TOKEN = '<ADDRESS>'\n",
    "URL_TOKEN = '<URL>'\n",
    "PHONE_TOKEN = '<PHONE>'\n",
    "SUP_TOKEN = '<SUP>'\n",
    "\n",
    "VOCAB_SIZE = len(CHAR2IDX)\n",
    "IDX2CHAR = {v: k for k, v in CHAR2IDX.items()}\n",
    "ic(VOCAB_SIZE)\n",
    "ic(len(IDX2CHAR))\n",
    "\n",
    "STATS = {}\n",
    "CLASSES = [\n",
    "  'address', \n",
    "  'url', \n",
    "  'phone', \n",
    "  'sup',\n",
    "  ]\n",
    "PHRASE_TYPES = dict(zip(CLASSES, range(len(CLASSES))))\n",
    "N_TYPES = len(CLASSES)\n",
    "MAX_PHRASE_LEN = 32\n",
    "\n",
    "def get_phrase_type(phrase):\n",
    "  # Phone Number\n",
    "  if re.match(r'^[\\d+-]+$', phrase):\n",
    "    return 'phone'\n",
    "  # url\n",
    "  elif any([substr in phrase for substr in ['www', '.', '/']\n",
    "           ]) and ' ' not in phrase:\n",
    "    return 'url'\n",
    "  # Address\n",
    "  else:\n",
    "    return 'address'\n",
    "\n",
    "def get_vocab_size():\n",
    "  vocab_size = VOCAB_SIZE\n",
    "  return vocab_size\n",
    "\n",
    "def get_n_cols(no_motion=False, use_z=None):\n",
    "  n_cols = N_COLS\n",
    "  if use_z is None:\n",
    "    use_z = FLAGS.use_z\n",
    "  \n",
    "  if FLAGS.concat_frames:\n",
    "    assert FLAGS.norm_frames\n",
    "    n_cols += N_COLS\n",
    "  \n",
    "  if not use_z:\n",
    "    n_cols = n_cols // 3 * 2\n",
    "    \n",
    "  if FLAGS.add_pos:\n",
    "    n_cols += 1\n",
    "  \n",
    "  return n_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f79c16",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.686891Z",
     "iopub.status.busy": "2023-08-27T08:15:35.685499Z",
     "iopub.status.idle": "2023-08-27T08:15:35.721506Z",
     "shell.execute_reply": "2023-08-27T08:15:35.720065Z"
    },
    "papermill": {
     "duration": 0.061334,
     "end_time": "2023-08-27T08:15:35.724807",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.663473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Tensorflow layer to process data in TFLite\n",
    "    Data needs to be processed in the model itself, so we can not use Python\n",
    "    [None, N_COLS] -> [1, NONE, N_COLS] -> [1, N_FRAMES, N_COLS]\n",
    "\"\"\"\n",
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, n_frames=128, training=False):\n",
    "    super(PreprocessLayer, self).__init__()\n",
    "    self.n_frames = n_frames\n",
    "    self.n_cols = get_n_cols(no_motion=True, use_z=True)\n",
    "    ic(self.n_cols)\n",
    "    self.means = np.load(f'../input/3rd-place-step-3-gen-mean-and-std/means.npy')\n",
    "    self.stds = np.load(f'../input/3rd-place-step-3-gen-mean-and-std/stds.npy')\n",
    "    ic(self.means.shape, self.stds.shape)\n",
    "    self.training = training\n",
    "\n",
    "  ## seems not needed as training perf is similar\n",
    "  @tf.function(input_signature=(tf.TensorSpec(shape=[None, N_COLS],\n",
    "                                              dtype=tf.float32),),)\n",
    "  ## this will cause error\n",
    "  # @tf.function()\n",
    "  def call(self, data):\n",
    "    trunct_method = FLAGS.trunct_method\n",
    "    training = self.training\n",
    "\n",
    "    # Hacky (add dim in front line shape [None,N_COLS] to shape [1, None, N_COLS])\n",
    "    data = data[None]\n",
    "    dtype = data.dtype\n",
    "    \n",
    "    # [1, None, N_COLS//3, 3]\n",
    "    data = reshape(data)\n",
    "    \n",
    "    if training and FLAGS.use_aug:\n",
    "      data = Apply(scale, FLAGS.scale_rate)(data)\n",
    "      data = Apply(rotate, FLAGS.rotate_rate)(data)\n",
    "      # shift not affect much..\n",
    "      data = Apply(shift, FLAGS.shift_rate)(data)\n",
    "      data = OneOf([scale, rotate, shift], FLAGS.affine_rate)(data)\n",
    "      data = Apply(temporal_seq_mask, FLAGS.temporal_seq_mask_rate)(data)\n",
    "    \n",
    "    data_ = data\n",
    "    data = reshape_back(data)\n",
    "      \n",
    "    # seems norm before resize produce better results\n",
    "    # -------norm frames\n",
    "    l = []\n",
    "    if FLAGS.norm_frames:\n",
    "      if FLAGS.concat_frames:\n",
    "        l.append(data_)\n",
    "      l.append(\n",
    "          (data - self.means) / self.stds,\n",
    "      )\n",
    "      l[-1] = reshape(l[-1])\n",
    "    else:\n",
    "      l.append(data_)\n",
    "            \n",
    "    # [1, None, feats, 3]\n",
    "    data = tf.concat(l, axis=-2)\n",
    "\n",
    "    n_cols = self.n_cols if not FLAGS.add_pos else self.n_cols - 1\n",
    "    data = tf.reshape(data, [1, -1, n_cols])\n",
    "    # Fill NaN Values With 0\n",
    "    data = tf.where(tf.math.is_nan(data), tf.zeros_like(data), data)\n",
    "                       \n",
    "    # add_pos helps a lot and add_pos_before_resample also +0.002 compare to add_pos_after_resample\n",
    "    if FLAGS.add_pos and FLAGS.add_pos_before_resample:\n",
    "      # n_frames = len(data[0])\n",
    "      n_frames = tf.shape(data)[1]\n",
    "      pos = get_pos(n_frames, dtype)\n",
    "      data = tf.concat([data, pos], axis=-1)\n",
    "      \n",
    "    #  now basic features done, we resample for frames with basic features\n",
    "    if training and FLAGS.use_aug:\n",
    "      # resample for time range, like 120 frames to 135 or 90 frames\n",
    "      # resample helps a lot\n",
    "      data = Apply(resample, FLAGS.resample_rate)(data)\n",
    "      data = tf.cast(data, dtype)\n",
    "      \n",
    "    # ------add addional info like add pos after resample, actually similar results before or after resample\n",
    "    # n_frames = len(data[0])\n",
    "    n_frames = tf.shape(data)[1]\n",
    "    \n",
    "    # do it after expand dim 0, and do not use expand_dims before...\n",
    "    if FLAGS.add_pos and (not FLAGS.add_pos_before_resample):\n",
    "      pos = get_pos(n_frames, dtype)\n",
    "      data = tf.concat([data, pos], axis=-1)\n",
    "\n",
    "    if not FLAGS.always_resize:\n",
    "      # Pad Zeros  TODO dynamic type so not pad? dynanmic seq2seq input\n",
    "      # n_frames = len(data[0])\n",
    "      n_frames = tf.shape(data)[1]\n",
    "      if n_frames < self.n_frames:\n",
    "        if FLAGS.pad_frames:\n",
    "          if FLAGS.pad_method == 'zero':\n",
    "            data = tf.concat((data,\n",
    "                              tf.zeros([1, self.n_frames - n_frames, self.n_cols],\n",
    "                                        dtype=dtype)),\n",
    "                            axis=1)\n",
    "          else:\n",
    "            data = tf.image.resize(\n",
    "              data,\n",
    "              [1, self.n_frames],\n",
    "              method=FLAGS.pad_resize_method,\n",
    "            )\n",
    "        elif n_frames < FLAGS.encode_pool_size:\n",
    "          data = tf.concat(\n",
    "              (data,\n",
    "               tf.zeros([1, FLAGS.encode_pool_size - n_frames, self.n_cols],\n",
    "                        dtype=dtype)),\n",
    "              axis=1)\n",
    "        elif n_frames < FLAGS.min_frames:\n",
    "          data = tf.concat(\n",
    "              (data,\n",
    "               tf.zeros([1, FLAGS.min_frames - n_frames, self.n_cols],\n",
    "                        dtype=dtype)),\n",
    "              axis=1)\n",
    "\n",
    "      # n_frames = len(data[0])\n",
    "      n_frames = tf.shape(data)[1]\n",
    "      if n_frames > self.n_frames:\n",
    "        if trunct_method == 'resize':\n",
    "          # Downsample\n",
    "          data = tf.image.resize(\n",
    "              data,\n",
    "              [1, self.n_frames],\n",
    "              method=FLAGS.resize_method,\n",
    "          )\n",
    "          # For resize The return value has type float32, unless the method is ResizeMethod.NEAREST_NEIGHBOR\n",
    "          data = tf.cast(data, dtype)\n",
    "        else:\n",
    "          data = data[:, :self.n_frames, :]\n",
    "    else:\n",
    "      data = tf.image.resize(\n",
    "          data,\n",
    "          [1, self.n_frames],\n",
    "          method=FLAGS.resize_method,\n",
    "      )\n",
    "      data = tf.cast(data, dtype)\n",
    "\n",
    "    if FLAGS.pad_frames:\n",
    "      data = tf.reshape(data, [1, self.n_frames, self.n_cols])\n",
    "      \n",
    "\n",
    "    if training and FLAGS.use_aug:\n",
    "      # temperal mask help a lot\n",
    "      data = Apply(temporal_mask, FLAGS.temporal_mask_rate)(data)\n",
    "      data = Apply(spatio_mask, FLAGS.spatio_mask_rate)(data)\n",
    "      \n",
    "    if not FLAGS.use_z:\n",
    "      if FLAGS.add_pos:\n",
    "        pos_data = data[...,-2:-1]\n",
    "        data = data[...,:-1]\n",
    "      data = tf.reshape(data, [1, self.n_frames, -1, 3])\n",
    "      data = data[...,:2]\n",
    "      data = tf.reshape(data, [1, self.n_frames, -1])\n",
    "      if FLAGS.add_pos:\n",
    "        data = tf.concat([data, pos_data], axis=-1)\n",
    "             \n",
    "    # Squeeze Batch Dimension\n",
    "    data = tf.squeeze(data, axis=[0])\n",
    "    # tf.print('----', data.shape)\n",
    "    return data\n",
    "\n",
    "class PreProcssor(object):\n",
    "  def __init__(self, subset='valid', squeeze=False):\n",
    "    training = subset == 'train'\n",
    "    self.prepocess = PreprocessLayer(FLAGS.n_frames, training=training)\n",
    "    self.squeeze = squeeze\n",
    "\n",
    "  def __call__(self, fe):\n",
    "    # # TODO HACK for hug datasets.. ds = ds.to_tf_dataset(batch_size=1)\n",
    "    if self.squeeze:\n",
    "      for key in fe:\n",
    "        fe[key] = tf.squeeze(fe[key], axis=0)\n",
    "    x = fe\n",
    "    weights = fe['weight']\n",
    "    fe['frames'] = tf.reshape(fe['frames'], [fe['n_frames'], N_COLS])\n",
    "    fe['frames'] = self.prepocess(fe['frames']) \n",
    "    fe['phrase_len'] = tf.cond(tf.greater(fe['phrase_len'], MAX_PHRASE_LEN), lambda: tf.constant(MAX_PHRASE_LEN, fe['phrase_len'].dtype), lambda: fe['phrase_len'])\n",
    "    if FLAGS.no_eos:\n",
    "      # as in tfrecords we have eos, but in training if we don't have eos we change it to pad\n",
    "      mask = tf.logical_or(fe['phrase_'] == PAD_IDX, fe['phrase_'] == EOS_IDX)\n",
    "      mask = tf.cast(mask, fe['phrase_'].dtype)\n",
    "      fe['phrase_'] = fe['phrase_'] * (1 - mask) + mask * PAD_IDX\n",
    "        \n",
    "    if FLAGS.mix_sup:\n",
    "      weights_mask = tf.cast(weights != 1, tf.float32)\n",
    "      weights = weights_mask * FLAGS.sup_weight + (1 - weights_mask)\n",
    "      x['weight'] = weights\n",
    "    \n",
    "    y = fe['phrase_']\n",
    "    return x, y\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c18a731",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.767632Z",
     "iopub.status.busy": "2023-08-27T08:15:35.765352Z",
     "iopub.status.idle": "2023-08-27T08:15:35.773717Z",
     "shell.execute_reply": "2023-08-27T08:15:35.772744Z"
    },
    "papermill": {
     "duration": 0.031239,
     "end_time": "2023-08-27T08:15:35.776212",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.744973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLAGS.use_aug = True\n",
    "FLAGS.always_resize = False\n",
    "FLAGS.affine_rate = 0\n",
    "FLAGS.force_flip = False\n",
    "FLAGS.scale_rate = 0.75\n",
    "FLAGS.scale_range = [0.8, 1.2]\n",
    "FLAGS.rotate_range = [-15.0, 15.0]\n",
    "FLAGS.shift_range = [-0.05, 0.05]\n",
    "FLAGS.temporal_seq_mask_range = [0.1, 0.2]\n",
    "FLAGS.pad_frames = True\n",
    "FLAGS.pad_method = 'zero'\n",
    "FLAGS.resize_method = 'bilinear'\n",
    "FLAGS.no_eos = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7c3ca21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.822585Z",
     "iopub.status.busy": "2023-08-27T08:15:35.822161Z",
     "iopub.status.idle": "2023-08-27T08:15:35.839230Z",
     "shell.execute_reply": "2023-08-27T08:15:35.837769Z"
    },
    "papermill": {
     "duration": 0.042325,
     "end_time": "2023-08-27T08:15:35.843562",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.801237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class TorchDataset(IterableDataset):\n",
    "  def __init__(self, subset='eval'):\n",
    "    self.subset = subset\n",
    "\n",
    "    if subset == 'train':\n",
    "      dataset = TfDataset('train', files=FLAGS.train_files)\n",
    "      datas = dataset.make_batch(FLAGS.batch_size, \n",
    "                             shuffle=True,\n",
    "                             repeat=True,\n",
    "                             drop_remainder=True,\n",
    "                             return_numpy=True)\n",
    "    else:\n",
    "      dataset = TfDataset('valid', files=FLAGS.valid_files)\n",
    "      datas = dataset.make_batch(FLAGS.eval_batch_size,  \n",
    "                             shuffle=False,\n",
    "                             repeat=False, \n",
    "                             drop_remainder=False,\n",
    "                             return_numpy=False) # if set return numpy = True then you could only visit 1 time..\n",
    "    \n",
    "    self.num_instances = dataset.num_instances\n",
    "    ic(self.num_instances)\n",
    "    self.num_steps = dataset.num_steps\n",
    "    ic(self.num_steps)\n",
    "    \n",
    "    self.datas = datas\n",
    "    self.data_iter = iter(datas)\n",
    "         \n",
    "  def __iter__(self):\n",
    "    if self.subset == 'train':\n",
    "      while True:\n",
    "        x, y = next(self.data_iter)\n",
    "        del x['phrase_type']\n",
    "        del x['phrase']\n",
    "        if FLAGS.distributed:\n",
    "          rank = gezi.get('RANK', 0)\n",
    "          start = rank * FLAGS.batch_size\n",
    "          end = start + FLAGS.batch_size\n",
    "          for key in x:\n",
    "            x[key] = x[key][start:end]\n",
    "          y = y[start:end]\n",
    "        yield x, y\n",
    "    else:\n",
    "      for x, y in self.datas:\n",
    "        for key in x:\n",
    "          x[key] = x[key].numpy()\n",
    "        y = y.numpy()\n",
    "        yield x, y\n",
    "    \n",
    "  def __len__(self):\n",
    "    return self.num_instances\n",
    "  \n",
    "def get_dataloaders():\n",
    "  kwargs = {\n",
    "      'num_workers': 0, # >0 will hang...\n",
    "      'pin_memory': True,\n",
    "      'persistent_workers': False,\n",
    "  }\n",
    "  \n",
    "  train_ds = TorchDataset('train')\n",
    "  eval_ds = TorchDataset('eval')\n",
    "  \n",
    "  train_dl = torch.utils.data.DataLoader(train_ds,\n",
    "                                         batch_size=None,\n",
    "                                         sampler=None,\n",
    "                                         **kwargs)\n",
    "  eval_dl = torch.utils.data.DataLoader(eval_ds,\n",
    "                                        batch_size=None,\n",
    "                                        sampler=None,\n",
    "                                        **kwargs)\n",
    "\n",
    "  return train_dl, eval_dl  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdf5fd9",
   "metadata": {
    "papermill": {
     "duration": 0.021434,
     "end_time": "2023-08-27T08:15:35.886273",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.864839",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model(17 layers squeezeformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c25daea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.928331Z",
     "iopub.status.busy": "2023-08-27T08:15:35.927416Z",
     "iopub.status.idle": "2023-08-27T08:15:35.938004Z",
     "shell.execute_reply": "2023-08-27T08:15:35.936557Z"
    },
    "papermill": {
     "duration": 0.03486,
     "end_time": "2023-08-27T08:15:35.940352",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.905492",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLAGS.encode_pool_size = 2\n",
    "FLAGS.emb_batchnorm = True\n",
    "FLAGS.attn_drop = 0\n",
    "FLAGS.ff_drop = 0\n",
    "FLAGS.conv_drop = 0\n",
    "FLAGS.inst_drop_rate = 0.2\n",
    "FLAGS.cls_drop = 0.1\n",
    "FLAGS.conv1d_ksize_vals = [15]\n",
    "FLAGS.conv1d_expansion_factor = 2\n",
    "FLAGS.ff_mult = 4\n",
    "FLAGS.encoder_units = 200 \n",
    "FLAGS.encoder_layers = 17\n",
    "FLAGS.mhatt_dimhead = 32\n",
    "FLAGS.mhatt_heads = 8\n",
    "FLAGS.skip_factor = 0.5\n",
    "# time reduce from layer 8, means 7 + 10 = 17 layers while the first 7 layers using 320 frames and the last 10 layers using 160 frames\n",
    "FLAGS.time_reduce = True\n",
    "FLAGS.time_reduce_idx = 7\n",
    "FLAGS.time_kernel_size = 5 \n",
    "FLAGS.time_stride = 2\n",
    "# for ROPE\n",
    "FLAGS.scaling_type = 'dynamic'\n",
    "FLAGS.scaling_factor = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd91d9a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:35.981947Z",
     "iopub.status.busy": "2023-08-27T08:15:35.980180Z",
     "iopub.status.idle": "2023-08-27T08:15:35.997213Z",
     "shell.execute_reply": "2023-08-27T08:15:35.995890Z"
    },
    "papermill": {
     "duration": 0.040875,
     "end_time": "2023-08-27T08:15:35.999992",
     "exception": false,
     "start_time": "2023-08-27T08:15:35.959117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reshape(data):\n",
    "  data = tf.reshape(data, (1, -1, N_COLS // 3, 3))\n",
    "  return data\n",
    "\n",
    "def reshape_back(data):\n",
    "  data = tf.reshape(data, (1, -1, N_COLS))\n",
    "  return data\n",
    "\n",
    "def get_pos(n_frames, dtype):\n",
    "  pos = tf.range(n_frames, dtype=dtype)\n",
    "  pos /= 1000.\n",
    "  pos = tf.reshape(pos, [1, n_frames, 1])\n",
    "  return pos\n",
    "\n",
    "def simple_decode(pred):\n",
    "  x = tf.argmax(pred, axis=-1)\n",
    "  return x\n",
    "\n",
    "def adjust_pad(pred):\n",
    "  pred = tf.nn.softmax(pred, axis=-1)\n",
    "  pred0, pred1 = pred[..., 0:1], pred[..., 1:]\n",
    "  pred0 *= tf.cast(pred0 > FLAGS.pad_thre, pred.dtype)\n",
    "  pred = tf.concat([pred0, pred1], axis=-1)\n",
    "  return pred\n",
    "\n",
    "def ctc_decode(x):\n",
    "  x = tf.concat([x, tf.zeros((1), dtype=x.dtype)], axis=0)\n",
    "  diff = tf.not_equal(x[:-1], x[1:])\n",
    "  adjacent_indices = tf.where(diff)[:, 0]\n",
    "  x = tf.gather(x, adjacent_indices)\n",
    "  mask = x != PAD_IDX\n",
    "  x = tf.boolean_mask(x, mask, axis=0)\n",
    "  return x\n",
    "\n",
    "@tf.function()\n",
    "def decode_phrase(pred):\n",
    "  pred = adjust_pad(pred)  \n",
    "  x = tf.argmax(pred, axis=-1)\n",
    "  x = ctc_decode(x)\n",
    "  return x  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22bef0bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.040219Z",
     "iopub.status.busy": "2023-08-27T08:15:36.039175Z",
     "iopub.status.idle": "2023-08-27T08:15:36.050818Z",
     "shell.execute_reply": "2023-08-27T08:15:36.049542Z"
    },
    "papermill": {
     "duration": 0.03498,
     "end_time": "2023-08-27T08:15:36.053818",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.018838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "  def __init__(self, num_features, momentum=0.1, eps=1e-5):\n",
    "    super().__init__()\n",
    "    self.bn = nn.BatchNorm1d(num_features, momentum=momentum, eps=eps)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 2, 1)\n",
    "    x = self.bn(x)\n",
    "    x = x.permute(0, 2, 1)\n",
    "    return x\n",
    "\n",
    "class InstanceDropout(nn.Module):\n",
    "  def __init__(self, p=0.5):\n",
    "    super().__init__()\n",
    "    self.p = p\n",
    "    self.dropout = nn.Dropout(p)\n",
    "\n",
    "  def forward(self, x):\n",
    "    mask = torch.ones_like(x[:,:1,:1])\n",
    "    mask = self.dropout(mask)\n",
    "    return x * mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfc14284",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.095133Z",
     "iopub.status.busy": "2023-08-27T08:15:36.093855Z",
     "iopub.status.idle": "2023-08-27T08:15:36.104466Z",
     "shell.execute_reply": "2023-08-27T08:15:36.102916Z"
    },
    "papermill": {
     "duration": 0.034128,
     "end_time": "2023-08-27T08:15:36.107216",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.073088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleEmbedding(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.emb_batchnorm = True if FLAGS.emb_batchnorm is None else FLAGS.emb_batchnorm\n",
    "    self.embedding = nn.Linear(get_n_cols(), FLAGS.encoder_units, bias=False)\n",
    "    if self.emb_batchnorm:\n",
    "      self.batch_norm = BatchNorm(FLAGS.encoder_units, momentum=0.05, eps=1e-3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    if self.emb_batchnorm:\n",
    "      x = self.batch_norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aff1263a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.156737Z",
     "iopub.status.busy": "2023-08-27T08:15:36.155875Z",
     "iopub.status.idle": "2023-08-27T08:15:36.166880Z",
     "shell.execute_reply": "2023-08-27T08:15:36.165467Z"
    },
    "papermill": {
     "duration": 0.043371,
     "end_time": "2023-08-27T08:15:36.169699",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.126328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SqueezeformerEncoder(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.embedding = SimpleEmbedding()\n",
    "    attn_dropout, ff_dropout, conv_dropout = FLAGS.attn_drop, FLAGS.ff_drop, FLAGS.conv_drop\n",
    "    self.encoder = Squeezeformer(\n",
    "        dim=FLAGS.encoder_units,\n",
    "        depth=FLAGS.encoder_layers,\n",
    "        dim_head=FLAGS.mhatt_dimhead,\n",
    "        heads=FLAGS.mhatt_heads,\n",
    "        ff_mult=FLAGS.ff_mult,\n",
    "        conv_expansion_factor=FLAGS.conv1d_expansion_factor,  # 2\n",
    "        conv_kernel_size=FLAGS.conv1d_ksize_vals[0],\n",
    "        attn_dropout=attn_dropout,\n",
    "        ff_dropout=ff_dropout,\n",
    "        conv_dropout=conv_dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.embedding(x)\n",
    "    x = self.encoder(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2b2528f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.209613Z",
     "iopub.status.busy": "2023-08-27T08:15:36.209008Z",
     "iopub.status.idle": "2023-08-27T08:15:36.217580Z",
     "shell.execute_reply": "2023-08-27T08:15:36.216344Z"
    },
    "papermill": {
     "duration": 0.031537,
     "end_time": "2023-08-27T08:15:36.219956",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.188419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AvgPoolingModule(nn.Module):\n",
    "  def __init__(self, pool_size, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)\n",
    "    self.pooling = nn.AvgPool1d(pool_size)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = x.permute(0, 2, 1)\n",
    "    x = self.pooling(x)\n",
    "    x = x.permute(0, 2, 1)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c799b57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.261022Z",
     "iopub.status.busy": "2023-08-27T08:15:36.260541Z",
     "iopub.status.idle": "2023-08-27T08:15:36.267336Z",
     "shell.execute_reply": "2023-08-27T08:15:36.266006Z"
    },
    "papermill": {
     "duration": 0.031135,
     "end_time": "2023-08-27T08:15:36.269742",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.238607",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.encoder = SqueezeformerEncoder()\n",
    "    if FLAGS.encode_pool_size > 1:\n",
    "      self.pooling = AvgPoolingModule(FLAGS.encode_pool_size)\n",
    "\n",
    "  def forward(self, frames):\n",
    "    x = self.encoder(frames)  \n",
    "    if FLAGS.encode_pool_size > 1:\n",
    "      x = self.pooling(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa2adf36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.310035Z",
     "iopub.status.busy": "2023-08-27T08:15:36.309087Z",
     "iopub.status.idle": "2023-08-27T08:15:36.315787Z",
     "shell.execute_reply": "2023-08-27T08:15:36.314903Z"
    },
    "papermill": {
     "duration": 0.029841,
     "end_time": "2023-08-27T08:15:36.318412",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.288571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferModel(nn.Module):\n",
    "  def __init__(self, model, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.model = model\n",
    "  \n",
    "  def forward(self, frames):\n",
    "    res = self.model.infer(frames)\n",
    "    return res\n",
    "  \n",
    "  def infer(self, frames):\n",
    "    return self.forward(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81f690d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.359619Z",
     "iopub.status.busy": "2023-08-27T08:15:36.358733Z",
     "iopub.status.idle": "2023-08-27T08:15:36.372616Z",
     "shell.execute_reply": "2023-08-27T08:15:36.371508Z"
    },
    "papermill": {
     "duration": 0.038536,
     "end_time": "2023-08-27T08:15:36.376434",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.337898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.encoder = Encoder()\n",
    "    self.classifer = nn.Sequential(\n",
    "            nn.Dropout(FLAGS.cls_drop),\n",
    "            nn.Linear(FLAGS.encoder_units, get_vocab_size()),\n",
    "        )\n",
    "      \n",
    "  # TODO check training flag ok\n",
    "  def encode(self, frames):\n",
    "    return self.encoder(frames)\n",
    "\n",
    "  def forward_(self, frames):\n",
    "    x = self.encode(frames)\n",
    "    x = self.classifer(x)\n",
    "    return x\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    if self.training:\n",
    "      self.input_ = inputs\n",
    "    x = self.forward_(inputs['frames'])\n",
    "    res = {\n",
    "      'pred': x,\n",
    "    }\n",
    "    return res\n",
    "  \n",
    "  def infer(self, frames):\n",
    "    return self.forward_(frames)\n",
    "\n",
    "  def get_loss_fn(self):  \n",
    "    def ctc_loss(loss_obj, preds, labels, labels_lengths, weights=None):\n",
    "      preds = F.log_softmax(preds, dim=-1)\n",
    "      preds_lengths = torch.sum(torch.ones_like(preds[:,:,0]).long(), dim=-1)\n",
    "      loss = loss_obj(preds.transpose(0, 1), labels, preds_lengths, labels_lengths)\n",
    "      if weights is not None:\n",
    "        loss = torch.mean(loss * weights)\n",
    "      return loss\n",
    "      \n",
    "    def loss_fn(res, labels, x, step=None, epoch=None, training=None):\n",
    "      scalars = {}\n",
    "      weights = None\n",
    "      reduction = 'mean'\n",
    "      if FLAGS.mix_sup:\n",
    "        weights = x['weight']\n",
    "        reduction = 'none'\n",
    "      loss_obj = nn.CTCLoss(zero_infinity=True, reduction=reduction)\n",
    "      preds = res['pred'].float()\n",
    "      labels = labels.float()\n",
    "      labels_lengths = torch.sum((labels != PAD_IDX).long(), dim=-1)\n",
    "      loss = ctc_loss(loss_obj, preds, labels, labels_lengths, weights)      \n",
    "      return loss\n",
    "\n",
    "    return loss_fn\n",
    "  \n",
    "  def get_infer_model(self):\n",
    "    return InferModel(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7090de",
   "metadata": {
    "papermill": {
     "duration": 0.018324,
     "end_time": "2023-08-27T08:15:36.414371",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.396047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Code for squeezeformer, notice most code copy/modify from NEMO, and for ROPE encoding copy from huggingface llma part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b1f23d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.454559Z",
     "iopub.status.busy": "2023-08-27T08:15:36.453661Z",
     "iopub.status.idle": "2023-08-27T08:15:36.465200Z",
     "shell.execute_reply": "2023-08-27T08:15:36.464014Z"
    },
    "papermill": {
     "duration": 0.034897,
     "end_time": "2023-08-27T08:15:36.467996",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.433099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simplfiy from NEMO\n",
    "class TimeReductionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Squeezeformer Time Reduction procedure. Downsamples the audio by `stride` in the time dimension.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): input dimension of MultiheadAttentionMechanism and PositionwiseFeedForward\n",
    "        out_dim (int): Output dimension of the module.\n",
    "        kernel_size (int): Conv kernel size for depthwise convolution in convolution module\n",
    "        stride (int): Downsampling factor in time dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, out_dim: int, kernel_size: int = 5, stride: int = 2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.out_dim = out_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        ## NOTICE modify here so can dived by 2...\n",
    "        # self.padding = max(0, self.kernel_size - self.stride) \n",
    "        ##  # like k=5, stride=2 here padding is 2 which make 320 -> 160 -> 80\n",
    "        self.padding = (self.kernel_size + 1) // self.stride - 1\n",
    "\n",
    "        self.dw_conv = nn.Conv1d(\n",
    "            in_channels=d_model,\n",
    "            out_channels=d_model,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=self.padding,\n",
    "            groups=d_model,\n",
    "        )\n",
    "\n",
    "        self.pw_conv = nn.Conv1d(\n",
    "            in_channels=d_model, out_channels=out_dim, kernel_size=1, stride=1, padding=0, groups=1,\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # [B, C, T]\n",
    "        x = self.dw_conv(x)\n",
    "        x = self.pw_conv(x)\n",
    "        x = x.transpose(1, 2)  # [B, T, C]\n",
    "        return x\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        dw_max = self.kernel_size ** -0.5\n",
    "        pw_max = self.d_model ** -0.5\n",
    "\n",
    "        with torch.no_grad():\n",
    "            torch.nn.init.uniform_(self.dw_conv.weight, -dw_max, dw_max)\n",
    "            torch.nn.init.uniform_(self.dw_conv.bias, -dw_max, dw_max)\n",
    "            torch.nn.init.uniform_(self.pw_conv.weight, -pw_max, pw_max)\n",
    "            torch.nn.init.uniform_(self.pw_conv.bias, -pw_max, pw_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0cbbedf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.509296Z",
     "iopub.status.busy": "2023-08-27T08:15:36.508372Z",
     "iopub.status.idle": "2023-08-27T08:15:36.548356Z",
     "shell.execute_reply": "2023-08-27T08:15:36.547399Z"
    },
    "papermill": {
     "duration": 0.064101,
     "end_time": "2023-08-27T08:15:36.551049",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.486948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Swish(nn.SiLU):\n",
    "    \"\"\"\n",
    "    Swish activation function introduced in 'https://arxiv.org/abs/1710.05941'\n",
    "    Mathematically identical to SiLU. See note in nn.SiLU for references.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "class CausalConv1D(nn.Conv1d):\n",
    "    \"\"\"\n",
    "    A causal version of nn.Conv1d where each step would have limited access to locations on its right or left\n",
    "    All arguments are the same as nn.Conv1d except padding.\n",
    "\n",
    "    If padding is set None, then paddings are set automatically to make it a causal convolution where each location would not see any steps on its right.\n",
    "\n",
    "    If padding is set as a list (size of 2), then padding[0] would be used as left padding and padding[1] as right padding.\n",
    "    It would make it possible to control the number of steps to be accessible on the right and left.\n",
    "    This mode is not supported when stride > 1. padding[0]+padding[1] should be equal to (kernel_size - 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: Union[str, int] = 0,\n",
    "        dilation: int = 1,\n",
    "        groups: int = 1,\n",
    "        bias: bool = True,\n",
    "        padding_mode: str = 'zeros',\n",
    "        device=None,\n",
    "        dtype=None,\n",
    "    ) -> None:\n",
    "        self.cache_drop_size = None\n",
    "        if padding is None:\n",
    "            self._left_padding = kernel_size - 1\n",
    "            self._right_padding = stride - 1\n",
    "        else:\n",
    "            if stride != 1 and padding != kernel_size - 1:\n",
    "                raise ValueError(\"No striding allowed for non-symmetric convolutions!\")\n",
    "            if isinstance(padding, int):\n",
    "                self._left_padding = padding\n",
    "                self._right_padding = padding\n",
    "            elif isinstance(padding, list) and len(padding) == 2 and padding[0] + padding[1] == kernel_size - 1:\n",
    "                self._left_padding = padding[0]\n",
    "                self._right_padding = padding[1]\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid padding param: {padding}!\")\n",
    "\n",
    "        self._max_cache_len = self._left_padding\n",
    "\n",
    "        super(CausalConv1D, self).__init__(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=stride,\n",
    "            padding=0,\n",
    "            dilation=dilation,\n",
    "            groups=groups,\n",
    "            bias=bias,\n",
    "            padding_mode=padding_mode,\n",
    "            device=device,\n",
    "            dtype=dtype,\n",
    "        )\n",
    "\n",
    "    def update_cache(self, x, cache=None):\n",
    "        if cache is None:\n",
    "            new_x = F.pad(x, pad=(self._left_padding, self._right_padding))\n",
    "            next_cache = cache\n",
    "        else:\n",
    "            new_x = F.pad(x, pad=(0, self._right_padding))\n",
    "            new_x = torch.cat([cache, new_x], dim=-1)\n",
    "            if self.cache_drop_size > 0:\n",
    "                next_cache = new_x[:, :, : -self.cache_drop_size]\n",
    "            else:\n",
    "                next_cache = new_x\n",
    "            next_cache = next_cache[:, :, -cache.size(-1) :]\n",
    "        return new_x, next_cache\n",
    "\n",
    "    def forward(self, x, cache=None):\n",
    "        x, cache = self.update_cache(x, cache=cache)\n",
    "        x = super().forward(x)\n",
    "        if cache is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x, cache\n",
    "\n",
    "class ConformerFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    feed-forward module of Conformer model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout, activation=Swish()):\n",
    "        super(ConformerFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = activation\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def reset_parameters_ff(self):\n",
    "        ffn1_max = self.d_model ** -0.5\n",
    "        ffn2_max = self.d_ff ** -0.5\n",
    "        with torch.no_grad():\n",
    "            nn.init.uniform_(self.linear1.weight, -ffn1_max, ffn1_max)\n",
    "            nn.init.uniform_(self.linear1.bias, -ffn1_max, ffn1_max)\n",
    "            nn.init.uniform_(self.linear2.weight, -ffn2_max, ffn2_max)\n",
    "            nn.init.uniform_(self.linear2.bias, -ffn2_max, ffn2_max)\n",
    "\n",
    "activation_registry = {\n",
    "    \"identity\": nn.Identity,\n",
    "    \"hardtanh\": nn.Hardtanh,\n",
    "    \"relu\": nn.ReLU,\n",
    "    \"selu\": nn.SELU,\n",
    "    \"swish\": nn.SiLU,\n",
    "    \"silu\": nn.SiLU,\n",
    "    \"gelu\": nn.GELU,\n",
    "}\n",
    "\n",
    "class ConformerConvolution(nn.Module):\n",
    "    \"\"\"The convolution module for the Conformer model.\n",
    "    Args:\n",
    "        d_model (int): hidden dimension\n",
    "        kernel_size (int): kernel size for depthwise convolution\n",
    "        pointwise_activation (str): name of the activation function to be used for the pointwise conv.\n",
    "            Note that Conformer uses a special key `glu_` which is treated as the original default from\n",
    "            the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, d_model, kernel_size, norm_type='batch_norm', conv_context_size=None, pointwise_activation='glu_'\n",
    "    ):\n",
    "        super(ConformerConvolution, self).__init__()\n",
    "        assert (kernel_size - 1) % 2 == 0\n",
    "        self.d_model = d_model\n",
    "        self.kernel_size = kernel_size\n",
    "        self.norm_type = norm_type\n",
    "\n",
    "        if conv_context_size is None:\n",
    "            conv_context_size = (kernel_size - 1) // 2\n",
    "\n",
    "        if pointwise_activation in activation_registry:\n",
    "            self.pointwise_activation = activation_registry[pointwise_activation]()\n",
    "            dw_conv_input_dim = d_model * 2\n",
    "\n",
    "            if hasattr(self.pointwise_activation, 'inplace'):\n",
    "                self.pointwise_activation.inplace = True\n",
    "        else:\n",
    "            self.pointwise_activation = pointwise_activation\n",
    "            dw_conv_input_dim = d_model\n",
    "\n",
    "        self.pointwise_conv1 = nn.Conv1d(\n",
    "            in_channels=d_model, out_channels=d_model * 2, kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "\n",
    "        self.depthwise_conv = CausalConv1D(\n",
    "            in_channels=dw_conv_input_dim,\n",
    "            out_channels=dw_conv_input_dim,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=conv_context_size,\n",
    "            groups=dw_conv_input_dim,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        if norm_type == 'batch_norm':\n",
    "            self.batch_norm = nn.BatchNorm1d(dw_conv_input_dim)\n",
    "        elif norm_type == 'instance_norm':\n",
    "            self.batch_norm = nn.InstanceNorm1d(dw_conv_input_dim)\n",
    "        elif norm_type == 'layer_norm':\n",
    "            self.batch_norm = nn.LayerNorm(dw_conv_input_dim)\n",
    "        elif norm_type.startswith('group_norm'):\n",
    "            num_groups = int(norm_type.replace(\"group_norm\", \"\"))\n",
    "            self.batch_norm = nn.GroupNorm(num_groups=num_groups, num_channels=d_model)\n",
    "        else:\n",
    "            raise ValueError(f\"conv_norm_type={norm_type} is not valid!\")\n",
    "\n",
    "        self.activation = Swish()\n",
    "        self.pointwise_conv2 = nn.Conv1d(\n",
    "            in_channels=dw_conv_input_dim, out_channels=d_model, kernel_size=1, stride=1, padding=0, bias=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x, pad_mask=None, cache=None):\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.pointwise_conv1(x)\n",
    "\n",
    "        # Compute the activation function or use GLU for original Conformer\n",
    "        if self.pointwise_activation == 'glu_':\n",
    "            x = nn.functional.glu(x, dim=1)\n",
    "        else:\n",
    "            x = self.pointwise_activation(x)\n",
    "\n",
    "        if pad_mask is not None:\n",
    "            x = x.float().masked_fill(pad_mask.unsqueeze(1), 0.0)\n",
    "\n",
    "        x = self.depthwise_conv(x, cache=cache)\n",
    "        if cache is not None:\n",
    "            x, cache = x\n",
    "\n",
    "        if self.norm_type == \"layer_norm\":\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.batch_norm(x)\n",
    "            x = x.transpose(1, 2)\n",
    "        else:\n",
    "            x = self.batch_norm(x)\n",
    "\n",
    "        x = self.activation(x)\n",
    "        x = self.pointwise_conv2(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        if cache is None:\n",
    "            return x\n",
    "        else:\n",
    "            return x, cache\n",
    "\n",
    "    def reset_parameters_conv(self):\n",
    "        pw1_max = pw2_max = self.d_model ** -0.5\n",
    "        dw_max = self.kernel_size ** -0.5\n",
    "\n",
    "        with torch.no_grad():\n",
    "            nn.init.uniform_(self.pointwise_conv1.weight, -pw1_max, pw1_max)\n",
    "            nn.init.uniform_(self.pointwise_conv1.bias, -pw1_max, pw1_max)\n",
    "            nn.init.uniform_(self.pointwise_conv2.weight, -pw2_max, pw2_max)\n",
    "            nn.init.uniform_(self.pointwise_conv2.bias, -pw2_max, pw2_max)\n",
    "            nn.init.uniform_(self.depthwise_conv.weight, -dw_max, dw_max)\n",
    "            nn.init.uniform_(self.depthwise_conv.bias, -dw_max, dw_max)\n",
    "            \n",
    "class ScaleBiasLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes an affine transformation y = x * scale + bias, either learned via adaptive weights, or fixed.\n",
    "    Efficient alternative to LayerNorm where we can avoid computing the mean and variance of the input, and\n",
    "    just rescale the output of the previous layer.\n",
    "\n",
    "    Args:\n",
    "        d_model (int): input dimension of layer.\n",
    "        adaptive_scale (bool): whether to learn the affine transformation parameters or not. If set to False,\n",
    "            the scale is fixed to 1 and bias to 0, effectively performing a No-Op on the input.\n",
    "            This is done for export compatibility.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model: int, adaptive_scale: bool):\n",
    "        super().__init__()\n",
    "        self.adaptive_scale = adaptive_scale\n",
    "        if adaptive_scale:\n",
    "            self.scale = nn.Parameter(torch.ones(d_model))\n",
    "            self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "        else:\n",
    "            self.register_buffer('scale', torch.ones(d_model), persistent=True)\n",
    "            self.register_buffer('bias', torch.zeros(d_model), persistent=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        scale = self.scale.view(1, 1, -1)\n",
    "        bias = self.bias.view(1, 1, -1)\n",
    "        return x * scale + bias\n",
    "\n",
    "class DepthWiseConv1d(nn.Module):\n",
    "\n",
    "  def __init__(self, chan_in, chan_out, kernel_size, padding):\n",
    "    super().__init__()\n",
    "    self.padding = padding\n",
    "    self.conv = nn.Conv1d(chan_in, chan_out, kernel_size, groups=chan_in)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = F.pad(x, self.padding)\n",
    "    return self.conv(x)\n",
    "  \n",
    "# attention, feedforward, and conv module\n",
    "\n",
    "\n",
    "class Scale(nn.Module):\n",
    "\n",
    "  def __init__(self, scale, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn\n",
    "    self.scale = scale\n",
    "\n",
    "  def forward(self, x, **kwargs):\n",
    "    return self.fn(x, **kwargs) * self.scale\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "\n",
    "  def __init__(self, dim, fn):\n",
    "    super().__init__()\n",
    "    self.fn = fn\n",
    "    self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "  def forward(self, x, **kwargs):\n",
    "    x = self.norm(x)\n",
    "    return self.fn(x, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2b750",
   "metadata": {
    "papermill": {
     "duration": 0.018237,
     "end_time": "2023-08-27T08:15:36.587757",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.569520",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ROPE code from huggingface llma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bb23fed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.627873Z",
     "iopub.status.busy": "2023-08-27T08:15:36.626960Z",
     "iopub.status.idle": "2023-08-27T08:15:36.649931Z",
     "shell.execute_reply": "2023-08-27T08:15:36.648898Z"
    },
    "papermill": {
     "duration": 0.046267,
     "end_time": "2023-08-27T08:15:36.652698",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.606431",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rotary positional embedding\n",
    "# https://arxiv.org/abs/2104.09864\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py\n",
    "class LlamaRotaryEmbedding(torch.nn.Module):\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.base = base\n",
    "        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        # Build here to make `torch.jit.trace` work.\n",
    "        self._set_cos_sin_cache(\n",
    "            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()\n",
    "        )\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        # freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        \n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "\n",
    "    def forward(self, x, seq_len=None):\n",
    "        # x: [bs, num_attention_heads, seq_len, head_size]\n",
    "        if seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n",
    "        )\n",
    "\n",
    "\n",
    "class LlamaLinearScalingRotaryEmbedding(LlamaRotaryEmbedding):\n",
    "    \"\"\"LlamaRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base, device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "        t = t / self.scaling_factor\n",
    "\n",
    "        # freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "\n",
    "\n",
    "class LlamaDynamicNTKScalingRotaryEmbedding(LlamaRotaryEmbedding):\n",
    "    \"\"\"LlamaRotaryEmbedding extended with Dynamic NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\"\"\"\n",
    "\n",
    "    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, scaling_factor=1.0):\n",
    "        self.scaling_factor = scaling_factor\n",
    "        super().__init__(dim, max_position_embeddings, base, device)\n",
    "\n",
    "    def _set_cos_sin_cache(self, seq_len, device, dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "\n",
    "        if seq_len > self.max_position_embeddings:\n",
    "            base = self.base * (\n",
    "                (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n",
    "            ) ** (self.dim / (self.dim - 2))\n",
    "            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n",
    "            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
    "\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        # freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
    "        freqs = torch.outer(t, self.inv_freq)\n",
    "        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :].to(dtype), persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :].to(dtype), persistent=False)\n",
    "\n",
    "\n",
    "def rotate_half(x):\n",
    "    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    # return torch.cat((-x2, x1), dim=-1)\n",
    "    # return torch.cat((torch.neg(x2), x1), dim=-1)\n",
    "    return torch.cat((x2 * (-1.), x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # The first two dimensions of cos and sin are always 1, so we can `squeeze` them.\n",
    "    cos = cos.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    sin = sin.squeeze(1).squeeze(0)  # [seq_len, dim]\n",
    "    cos = cos[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    sin = sin[position_ids].unsqueeze(1)  # [bs, 1, seq_len, dim]\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250d91f",
   "metadata": {
    "papermill": {
     "duration": 0.018248,
     "end_time": "2023-08-27T08:15:36.690559",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.672311",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Attention is important which use ROPE relative pos encodeing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f71d339a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.730686Z",
     "iopub.status.busy": "2023-08-27T08:15:36.729801Z",
     "iopub.status.idle": "2023-08-27T08:15:36.745826Z",
     "shell.execute_reply": "2023-08-27T08:15:36.744909Z"
    },
    "papermill": {
     "duration": 0.03924,
     "end_time": "2023-08-27T08:15:36.748468",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.709228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "  def __init__(self, dim, dim_head=64, \n",
    "                heads=8, dropout=0., max_pos_emb=512, \n",
    "                relpos_att=True, rope=False):\n",
    "    super().__init__()\n",
    "    self.scale = dim_head ** -0.5\n",
    "    self.heads = heads\n",
    "    inner_dim = heads * dim_head\n",
    "\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "    self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "    self.to_out = nn.Linear(inner_dim, dim, bias = False)\n",
    "    self.max_position_embeddings = max_pos_emb\n",
    "    self.dim_head = dim_head\n",
    "    self.relpos_att = relpos_att\n",
    "    self.rope = rope\n",
    "    if relpos_att:\n",
    "      if rope:\n",
    "        self._init_rope()\n",
    "      else:\n",
    "        self.rel_pos_emb = nn.Embedding(2 * max_pos_emb + 1, dim_head)\n",
    "\n",
    "  def _init_rope(self):\n",
    "    scaling_type = FLAGS.scaling_type\n",
    "    if scaling_type is None:\n",
    "        self.rotary_emb = LlamaRotaryEmbedding(self.dim_head, max_position_embeddings=self.max_position_embeddings)\n",
    "    else:\n",
    "        scaling_factor = FLAGS.scaling_factor\n",
    "        if scaling_type == \"linear\":\n",
    "            self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n",
    "                self.dim_head, max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor\n",
    "            )\n",
    "        elif scaling_type == \"dynamic\":\n",
    "            self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n",
    "                self.dim_head, max_position_embeddings=self.max_position_embeddings, scaling_factor=scaling_factor\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
    "      \n",
    "\n",
    "  def forward(self, x):\n",
    "    n, device, h = x.shape[-2], x.device, self.heads\n",
    "\n",
    "    q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "    \n",
    "    q = q.view(q.shape[0], q.shape[1], h, -1).permute(0, 2, 1, 3)\n",
    "    k = k.view(k.shape[0], k.shape[1], h, -1).permute(0, 2, 1, 3)\n",
    "    v = v.view(v.shape[0], v.shape[1], h, -1).permute(0, 2, 1, 3)\n",
    "\n",
    "    if self.relpos_att:\n",
    "      cos, sin = self.rotary_emb(v, seq_len=n)\n",
    "      position_ids = torch.arange(0, n, dtype=torch.long, device=device)\n",
    "      position_ids = position_ids.unsqueeze(0).view(-1, n)\n",
    "      q, k = apply_rotary_pos_emb(q, k, cos, sin, position_ids)\n",
    "      sim = torch.matmul(q, k.transpose(2, 3)) * self.scale\n",
    "    else:\n",
    "      sim = torch.matmul(q, k.permute(0, 1, 3, 2)) * self.scale\n",
    "    \n",
    "    attn = F.softmax(sim, dim=-1, dtype=torch.float32).to(q.dtype)\n",
    "    attn = self.dropout(attn)\n",
    "    \n",
    "    out = torch.matmul(attn, v)\n",
    "    out = out.permute(0, 2, 1, 3)\n",
    "    out = out.reshape(out.shape[0], out.shape[1], -1)\n",
    "    out = self.to_out(out)\n",
    "    out = self.dropout(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b45649c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.789109Z",
     "iopub.status.busy": "2023-08-27T08:15:36.788204Z",
     "iopub.status.idle": "2023-08-27T08:15:36.808636Z",
     "shell.execute_reply": "2023-08-27T08:15:36.807545Z"
    },
    "papermill": {
     "duration": 0.044329,
     "end_time": "2023-08-27T08:15:36.811512",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.767183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SqueezeformerBlock(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               *,\n",
    "               dim,\n",
    "               dim_head=64,\n",
    "               heads=8,\n",
    "               ff_mult=4,\n",
    "               conv_expansion_factor=2,\n",
    "               conv_kernel_size=31,\n",
    "               attn_dropout=0.,\n",
    "               ff_dropout=0.,\n",
    "               conv_dropout=0.,\n",
    "               conv_causal=False,\n",
    "               relpos_att=True,\n",
    "               rope=False,\n",
    "               inst_drop=None,\n",
    "               skip_factor=None):\n",
    "    super().__init__()\n",
    "    # first feed forward module\n",
    "    self.norm_feed_forward1 = nn.LayerNorm(dim)\n",
    "    self.feed_forward1 = ConformerFeedForward(d_model=dim, d_ff=dim * ff_mult, dropout=ff_dropout)\n",
    "    self.feed_forward1_scale = ScaleBiasLayer(d_model=dim, adaptive_scale=True)\n",
    "    \n",
    "    # convolution module\n",
    "    self.norm_conv = nn.LayerNorm(dim)\n",
    "    self.conv = ConformerConvolution(\n",
    "        d_model=dim, kernel_size=conv_kernel_size, norm_type='batch_norm', pointwise_activation='swish'\n",
    "    )\n",
    "    self.conv_scale = ScaleBiasLayer(d_model=dim, adaptive_scale=True)\n",
    "    \n",
    "    # multi-headed self-attention module\n",
    "    self.norm_self_att = nn.LayerNorm(dim)\n",
    "    self.self_attn = Attention(dim=dim,\n",
    "                          dim_head=dim_head,\n",
    "                          heads=heads,\n",
    "                          dropout=attn_dropout,\n",
    "                          max_pos_emb=FLAGS.n_frames,\n",
    "                          relpos_att=relpos_att,\n",
    "                          rope=rope)\n",
    "    self.self_attn_scale = ScaleBiasLayer(d_model=dim, adaptive_scale=True)\n",
    "\n",
    "    # second feed forward module\n",
    "    self.norm_feed_forward2 = nn.LayerNorm(dim)\n",
    "    self.feed_forward2 = ConformerFeedForward(d_model=dim, d_ff=dim * ff_mult, dropout=ff_dropout)\n",
    "    self.feed_forward2_scale = ScaleBiasLayer(d_model=dim, adaptive_scale=True)\n",
    "    \n",
    "    self.inst_drop = inst_drop if inst_drop is not None else FLAGS.inst_drop_rate\n",
    "    # 0.2\n",
    "    self.dropout = InstanceDropout(self.inst_drop)\n",
    "    self.skip_factor = skip_factor if skip_factor is not None else FLAGS.skip_factor\n",
    "    \n",
    "    self.fc_factor = 0.5\n",
    "    \n",
    "    self.reset_parameters()\n",
    "    \n",
    "  def reset_parameters(self):\n",
    "    # Used for Squeezeformer initialization only\n",
    "    self.feed_forward1.reset_parameters_ff()\n",
    "    self.feed_forward2.reset_parameters_ff()\n",
    "    self.conv.reset_parameters_conv()\n",
    "\n",
    "  def forward(self, x):\n",
    "    residual = x\n",
    "    x = self.self_attn_scale(x)\n",
    "    x = self.self_attn(x)\n",
    "    x = residual + self.dropout(x) * self.skip_factor\n",
    "    \n",
    "    x = self.norm_self_att(x)\n",
    "    residual = x\n",
    "    x = self.feed_forward1_scale(x)\n",
    "    x = self.feed_forward1(x)\n",
    "    x = residual + self.dropout(x) * self.skip_factor * self.fc_factor\n",
    "    x = self.norm_feed_forward1(x)\n",
    "    residual = x\n",
    "\n",
    "    x = self.conv_scale(x)\n",
    "    x = self.conv(x)\n",
    "    x = residual + self.dropout(x) * self.skip_factor\n",
    "    x = self.norm_conv(x)\n",
    "    residual = x\n",
    "    \n",
    "    x = self.feed_forward2_scale(x)\n",
    "    x = self.feed_forward2(x)\n",
    "    x = residual + self.dropout(x) * self.skip_factor * self.fc_factor\n",
    "    x = self.norm_feed_forward2(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class Squeezeformer(nn.Module):\n",
    "\n",
    "  def __init__(self,\n",
    "               dim,\n",
    "               *,\n",
    "               depth,\n",
    "               dim_head=64,\n",
    "               heads=8,\n",
    "               ff_mult=4,\n",
    "               conv_expansion_factor=2,\n",
    "               conv_kernel_size=31,\n",
    "               attn_dropout=0.,\n",
    "               ff_dropout=0.,\n",
    "               conv_dropout=0.,\n",
    "               conv_causal=False):\n",
    "    super().__init__()\n",
    "    self.dim = dim\n",
    "    self.layers = nn.ModuleList([])\n",
    "    self.inst_drops = [None] * depth\n",
    "    heads_ = heads\n",
    "    conv_kernel_size_ = conv_kernel_size\n",
    "    for i in range(depth):    \n",
    "      relpos_att = True\n",
    "      dim_head_ = dim_head\n",
    "      rope = True\n",
    "      heads_ = heads\n",
    "      if i < FLAGS.time_reduce_idx:\n",
    "        heads_ = heads // 2\n",
    "      \n",
    "      self.layers.append(SqueezeformerBlock(dim=dim,\n",
    "                          dim_head=dim_head_,\n",
    "                          heads=heads_,\n",
    "                          ff_mult=ff_mult,\n",
    "                          conv_expansion_factor=conv_expansion_factor,\n",
    "                          conv_kernel_size=conv_kernel_size_,\n",
    "                          conv_causal=conv_causal,\n",
    "                          attn_dropout=attn_dropout,\n",
    "                          ff_dropout=ff_dropout,\n",
    "                          conv_dropout=conv_dropout,\n",
    "                          relpos_att=relpos_att,\n",
    "                          rope=rope,\n",
    "                          inst_drop=self.inst_drops[i]))\n",
    "      \n",
    "    if FLAGS.time_reduce:\n",
    "      if FLAGS.time_reduce_idx < depth - 1:\n",
    "        reduction_module = TimeReductionModule(dim, dim, kernel_size=FLAGS.time_kernel_size, stride=FLAGS.time_stride) \n",
    "        self.layers.insert(FLAGS.time_reduce_idx, reduction_module)\n",
    "      \n",
    "  def forward(self, x):\n",
    "    for i, layer in enumerate(self.layers):\n",
    "      x = layer(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98ef15fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:36.851397Z",
     "iopub.status.busy": "2023-08-27T08:15:36.850539Z",
     "iopub.status.idle": "2023-08-27T08:15:37.388580Z",
     "shell.execute_reply": "2023-08-27T08:15:37.387473Z"
    },
    "papermill": {
     "duration": 0.560803,
     "end_time": "2023-08-27T08:15:37.391136",
     "exception": false,
     "start_time": "2023-08-27T08:15:36.830333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (encoder): Encoder(\n",
       "    (encoder): SqueezeformerEncoder(\n",
       "      (embedding): SimpleEmbedding(\n",
       "        (embedding): Linear(in_features=769, out_features=200, bias=False)\n",
       "        (batch_norm): BatchNorm(\n",
       "          (bn): BatchNorm1d(200, eps=0.001, momentum=0.05, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (encoder): Squeezeformer(\n",
       "        (layers): ModuleList(\n",
       "          (0-6): 7 x SqueezeformerBlock(\n",
       "            (norm_feed_forward1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward1): ConformerFeedForward(\n",
       "              (linear1): Linear(in_features=200, out_features=800, bias=True)\n",
       "              (activation): Swish()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=800, out_features=200, bias=True)\n",
       "            )\n",
       "            (feed_forward1_scale): ScaleBiasLayer()\n",
       "            (norm_conv): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (conv): ConformerConvolution(\n",
       "              (pointwise_activation): SiLU(inplace=True)\n",
       "              (pointwise_conv1): Conv1d(200, 400, kernel_size=(1,), stride=(1,))\n",
       "              (depthwise_conv): CausalConv1D(400, 400, kernel_size=(15,), stride=(1,), groups=400)\n",
       "              (batch_norm): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): Swish()\n",
       "              (pointwise_conv2): Conv1d(400, 200, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (conv_scale): ScaleBiasLayer()\n",
       "            (norm_self_att): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attn): Attention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (to_qkv): Linear(in_features=200, out_features=384, bias=False)\n",
       "              (to_out): Linear(in_features=128, out_features=200, bias=False)\n",
       "              (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
       "            )\n",
       "            (self_attn_scale): ScaleBiasLayer()\n",
       "            (norm_feed_forward2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward2): ConformerFeedForward(\n",
       "              (linear1): Linear(in_features=200, out_features=800, bias=True)\n",
       "              (activation): Swish()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=800, out_features=200, bias=True)\n",
       "            )\n",
       "            (feed_forward2_scale): ScaleBiasLayer()\n",
       "            (dropout): InstanceDropout(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): TimeReductionModule(\n",
       "            (dw_conv): Conv1d(200, 200, kernel_size=(5,), stride=(2,), padding=(2,), groups=200)\n",
       "            (pw_conv): Conv1d(200, 200, kernel_size=(1,), stride=(1,))\n",
       "          )\n",
       "          (8-17): 10 x SqueezeformerBlock(\n",
       "            (norm_feed_forward1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward1): ConformerFeedForward(\n",
       "              (linear1): Linear(in_features=200, out_features=800, bias=True)\n",
       "              (activation): Swish()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=800, out_features=200, bias=True)\n",
       "            )\n",
       "            (feed_forward1_scale): ScaleBiasLayer()\n",
       "            (norm_conv): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (conv): ConformerConvolution(\n",
       "              (pointwise_activation): SiLU(inplace=True)\n",
       "              (pointwise_conv1): Conv1d(200, 400, kernel_size=(1,), stride=(1,))\n",
       "              (depthwise_conv): CausalConv1D(400, 400, kernel_size=(15,), stride=(1,), groups=400)\n",
       "              (batch_norm): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (activation): Swish()\n",
       "              (pointwise_conv2): Conv1d(400, 200, kernel_size=(1,), stride=(1,))\n",
       "            )\n",
       "            (conv_scale): ScaleBiasLayer()\n",
       "            (norm_self_att): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attn): Attention(\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (to_qkv): Linear(in_features=200, out_features=768, bias=False)\n",
       "              (to_out): Linear(in_features=256, out_features=200, bias=False)\n",
       "              (rotary_emb): LlamaDynamicNTKScalingRotaryEmbedding()\n",
       "            )\n",
       "            (self_attn_scale): ScaleBiasLayer()\n",
       "            (norm_feed_forward2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
       "            (feed_forward2): ConformerFeedForward(\n",
       "              (linear1): Linear(in_features=200, out_features=800, bias=True)\n",
       "              (activation): Swish()\n",
       "              (dropout): Dropout(p=0, inplace=False)\n",
       "              (linear2): Linear(in_features=800, out_features=200, bias=True)\n",
       "            )\n",
       "            (feed_forward2_scale): ScaleBiasLayer()\n",
       "            (dropout): InstanceDropout(\n",
       "              (dropout): Dropout(p=0.2, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooling): AvgPoolingModule(\n",
       "      (pooling): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    )\n",
       "  )\n",
       "  (classifer): Sequential(\n",
       "    (0): Dropout(p=0.1, inplace=False)\n",
       "    (1): Linear(in_features=200, out_features=61, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae8d5482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:37.432229Z",
     "iopub.status.busy": "2023-08-27T08:15:37.431811Z",
     "iopub.status.idle": "2023-08-27T08:15:38.005153Z",
     "shell.execute_reply": "2023-08-27T08:15:38.003884Z"
    },
    "papermill": {
     "duration": 0.597693,
     "end_time": "2023-08-27T08:15:38.008565",
     "exception": false,
     "start_time": "2023-08-27T08:15:37.410872",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=======================================================================================================================================\n",
       "Layer (type:depth-idx)                                                                Output Shape              Param #\n",
       "=======================================================================================================================================\n",
       "InferModel                                                                            [1, 80, 61]               --\n",
       "├─Model: 1-1                                                                          --                        --\n",
       "│    └─Encoder: 2-1                                                                   [1, 80, 200]              --\n",
       "│    │    └─SqueezeformerEncoder: 3-1                                                 [1, 160, 200]             16,781,400\n",
       "│    │    └─AvgPoolingModule: 3-2                                                     [1, 80, 200]              --\n",
       "│    └─Sequential: 2-2                                                                [1, 80, 61]               --\n",
       "│    │    └─Dropout: 3-3                                                              [1, 80, 200]              --\n",
       "│    │    └─Linear: 3-4                                                               [1, 80, 61]               12,261\n",
       "=======================================================================================================================================\n",
       "Total params: 29,933,061\n",
       "Trainable params: 29,933,061\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 661.80\n",
       "=======================================================================================================================================\n",
       "Input size (MB): 0.98\n",
       "Forward/backward pass size (MB): 178.03\n",
       "Params size (MB): 67.17\n",
       "Estimated Total Size (MB): 246.19\n",
       "======================================================================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "input_shape = (1, FLAGS.n_frames, get_n_cols())\n",
    "summary(model.get_infer_model(), input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff987445",
   "metadata": {
    "papermill": {
     "duration": 0.019841,
     "end_time": "2023-08-27T08:15:38.049363",
     "exception": false,
     "start_time": "2023-08-27T08:15:38.029522",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TODO you may load weights of trained torch model here at first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "221d8ca8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:38.090368Z",
     "iopub.status.busy": "2023-08-27T08:15:38.089908Z",
     "iopub.status.idle": "2023-08-27T08:15:38.102196Z",
     "shell.execute_reply": "2023-08-27T08:15:38.100775Z"
    },
    "papermill": {
     "duration": 0.035997,
     "end_time": "2023-08-27T08:15:38.104781",
     "exception": false,
     "start_time": "2023-08-27T08:15:38.068784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TFLite model for submission\n",
    "class TFLiteModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, model):\n",
    "    super(TFLiteModel, self).__init__()\n",
    "\n",
    "    # Load the feature generation and main models\n",
    "    self.preprocess_layer = PreprocessLayer(FLAGS.n_frames)\n",
    "    self.model = model\n",
    "\n",
    "  @tf.function(jit_compile=True)\n",
    "  def infer(self, frames):\n",
    "    return self.model(frames)\n",
    "\n",
    "  @tf.function(input_signature=[\n",
    "      tf.TensorSpec(shape=[None, N_COLS], dtype=tf.float32, name='inputs')\n",
    "  ])\n",
    "  def call(self, inputs):\n",
    "    # Preprocess Data\n",
    "    frames_inp = self.preprocess_layer(inputs)\n",
    "    # x = frames_inp\n",
    "    # Add Batch Dimension\n",
    "    frames_inp = tf.expand_dims(frames_inp, axis=0)\n",
    "\n",
    "    outputs = self.infer(frames_inp)\n",
    "    # y = outputs\n",
    " \n",
    "    # Squeeze outputs\n",
    "    outputs = tf.squeeze(outputs, axis=0)\n",
    "    outputs = decode_phrase(outputs)\n",
    "    \n",
    "    # for 0 is PAD_IDX\n",
    "    outputs -= 1\n",
    "    # outputs = tf.one_hot(outputs, get_vocab_size())\n",
    "    # vocab_size = 61 if not FLAGS.no_eos else 60\n",
    "    vocab_size = get_vocab_size()\n",
    "    outputs = tf.one_hot(outputs, vocab_size)\n",
    "    if FLAGS.decode_phrase_type:\n",
    "      ouputs = outputs[1:]\n",
    "\n",
    "    # Return a dictionary with the output tensor\n",
    "    return {'outputs': outputs}\n",
    "    # return {\n",
    "    #   'outputs': outputs,\n",
    "    #   'frames': x,\n",
    "    #   'intermediate': y,\n",
    "    # }\n",
    "\n",
    "  # TODO not work... an intermediate Keras symbolic input/output, to a TF API that does not allow registering custom dispatchers, such as `tf.cond`, `tf.function`, gradient tapes, or `tf.map_fn`. Keras Functional model construction only supports TF API calls that *do* support dispatching, such as `tf.math.add` or `tf.reshape`. Other APIs cannot be called directly on symbolic Kerasinputs/outputs. You can work around this limitation by putting the operation in a custom Keras layer `call` and calling that layer on this symbolic input/output.\n",
    "  def get_model(self):\n",
    "    inputs = tf.keras.layers.Input([N_COLS],\n",
    "                                    dtype=tf.float32,\n",
    "                                    name='inputs')\n",
    "    out = self.call(inputs)\n",
    "    model = tf.keras.models.Model(inputs, out)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48d069ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:38.146556Z",
     "iopub.status.busy": "2023-08-27T08:15:38.145838Z",
     "iopub.status.idle": "2023-08-27T08:15:38.154915Z",
     "shell.execute_reply": "2023-08-27T08:15:38.153568Z"
    },
    "papermill": {
     "duration": 0.033254,
     "end_time": "2023-08-27T08:15:38.157295",
     "exception": false,
     "start_time": "2023-08-27T08:15:38.124041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLAGS.model_dir = './'\n",
    "\n",
    "def get_tflite_model(model):\n",
    "  tflite_keras_model = TFLiteModel(model)\n",
    "  return tflite_keras_model\n",
    "\n",
    "def to_tflite_model(model):\n",
    "  tflite_keras_model = get_tflite_model(model)\n",
    "  # tflite_keras_model = tflite_keras_model.get_model()\n",
    "  # plot_model(tflite_func_model, to_file=f'{FLAGS.model_dir}/tflite.png', show_shapes=True, show_layer_names=True)\n",
    "  ic(tflite_keras_model)\n",
    "  # Create Model Converter\n",
    "  converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "  converter.target_spec.supported_types = [tf.float16]\n",
    "  #  1/4 size but seems hurt acc a lot like 0.8 to 0.7\n",
    "  # converter.target_spec.supported_ops = [tf.lite.OpsSet.EXPERIMENTAL_TFLITE_BUILTINS_ACTIVATIONS_INT16_WEIGHTS_INT8,\n",
    "  #                                        tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "  converter._experimental_default_to_single_batch_in_tensor_list_ops = True\n",
    "  \n",
    "  # converter.post_training_quantize = True\n",
    "  ic(converter)\n",
    "  ## converter.experimental_new_converter = True\n",
    "  # Convert Model\n",
    "  tflite_model = converter.convert()\n",
    "  # ic(tflite_model)\n",
    "  # Write Model\n",
    "  with open(f'{FLAGS.model_dir}/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "  os.system(f'du -h {FLAGS.model_dir}/model.tflite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a336c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:38.199234Z",
     "iopub.status.busy": "2023-08-27T08:15:38.198762Z",
     "iopub.status.idle": "2023-08-27T08:15:50.518453Z",
     "shell.execute_reply": "2023-08-27T08:15:50.516799Z"
    },
    "papermill": {
     "duration": 12.344737,
     "end_time": "2023-08-27T08:15:50.522049",
     "exception": false,
     "start_time": "2023-08-27T08:15:38.177312",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sty\r\n",
      "  Downloading sty-1.0.4-py3-none-any.whl (11 kB)\r\n",
      "Installing collected packages: sty\r\n",
      "Successfully installed sty-1.0.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sty\n",
    "sys.path.append('../input/aslfr-nobuco/')\n",
    "import nobuco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88bc8ee6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:50.564709Z",
     "iopub.status.busy": "2023-08-27T08:15:50.564250Z",
     "iopub.status.idle": "2023-08-27T08:15:50.569956Z",
     "shell.execute_reply": "2023-08-27T08:15:50.568749Z"
    },
    "papermill": {
     "duration": 0.030322,
     "end_time": "2023-08-27T08:15:50.572261",
     "exception": false,
     "start_time": "2023-08-27T08:15:50.541939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLAGS.convert_trace = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc0c44c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:50.615589Z",
     "iopub.status.busy": "2023-08-27T08:15:50.615134Z",
     "iopub.status.idle": "2023-08-27T08:15:50.628737Z",
     "shell.execute_reply": "2023-08-27T08:15:50.627474Z"
    },
    "papermill": {
     "duration": 0.038736,
     "end_time": "2023-08-27T08:15:50.631134",
     "exception": false,
     "start_time": "2023-08-27T08:15:50.592398",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def torch2keras(model):\n",
    "  import nobuco\n",
    "  from nobuco import ChannelOrder, ChannelOrderingStrategy\n",
    "  from nobuco.layers.weight import WeightLayer\n",
    "  input_shape = [1, FLAGS.n_frames, get_n_cols()]\n",
    "  ic(input_shape)\n",
    "  dummy_input = torch.randn(input_shape)\n",
    "  model = model.get_infer_model()\n",
    "\n",
    "  # model = model.half().float()\n",
    "  model = model.to('cpu')\n",
    "  model = model.eval()\n",
    "  \n",
    "  ## TODO help merge to nobuco codebase\n",
    "  # for this you could use x = F.avg_pool1d(x, FLAGS.encode_pool_size)\n",
    "  @nobuco.converter(F.avg_pool1d, channel_ordering_strategy=ChannelOrderingStrategy.FORCE_TENSORFLOW_ORDER)\n",
    "  def converter_avg_pool1d(input: torch.Tensor, input2: int, inplace: bool = False):\n",
    "    return lambda input, input2, inplace=False: tf.keras.layers.AveragePooling1D(input2)(input)\n",
    "  \n",
    "  @nobuco.converter(nn.AvgPool1d, channel_ordering_strategy=ChannelOrderingStrategy.FORCE_TENSORFLOW_ORDER)\n",
    "  def converter_AvgPool1d(self, input: torch.Tensor):\n",
    "    return tf.keras.layers.AveragePooling1D(self.kernel_size)\n",
    "    \n",
    "  @nobuco.converter(torch.ones_like, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "  def converter_ones_like(input, *args, **kwargs):\n",
    "      def func(input, *args, **kwargs):\n",
    "          return tf.ones_like(input)\n",
    "      return func\n",
    "  \n",
    "  @nobuco.converter(torch.Tensor.bmm, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS_OR_PYTORCH, autocast=True)\n",
    "  def converter_bmm(self, value, *args, **kwargs):\n",
    "    def func(self, value, *args, **kwargs):\n",
    "        return tf.matmul(self, value)\n",
    "    return func\n",
    "  \n",
    "  @nobuco.converter(torch.flip, channel_ordering_strategy=ChannelOrderingStrategy.MINIMUM_TRANSPOSITIONS)\n",
    "  def converter_flip(input, input2, *args, **kwargs):\n",
    "      def func(input, input2, *args, **kwargs):\n",
    "          return tf.reverse(input, input2)\n",
    "      return func\n",
    "  \n",
    "  \n",
    "  input_shape[0] = None\n",
    "  keras_model = nobuco.pytorch_to_keras(\n",
    "      model,\n",
    "      args=[dummy_input], \n",
    "      input_shapes={dummy_input: input_shape},\n",
    "      inputs_channel_order=ChannelOrder.PYTORCH,\n",
    "      outputs_channel_order=ChannelOrder.PYTORCH,\n",
    "      trace_shape=True, \n",
    "      debug_traces=FLAGS.convert_trace,\n",
    "    )\n",
    "  keras_model.summary()\n",
    "  return keras_model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b1c3f",
   "metadata": {
    "papermill": {
     "duration": 0.019093,
     "end_time": "2023-08-27T08:15:50.669727",
     "exception": false,
     "start_time": "2023-08-27T08:15:50.650634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Convert torch to keras using nobuco  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ed47b28",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2023-08-27T08:15:50.711022Z",
     "iopub.status.busy": "2023-08-27T08:15:50.710521Z",
     "iopub.status.idle": "2023-08-27T08:16:45.083892Z",
     "shell.execute_reply": "2023-08-27T08:16:45.082449Z"
    },
    "papermill": {
     "duration": 54.397984,
     "end_time": "2023-08-27T08:16:45.087049",
     "exception": false,
     "start_time": "2023-08-27T08:15:50.689065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "keras_model = torch2keras(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c76c42b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-27T08:16:45.131955Z",
     "iopub.status.busy": "2023-08-27T08:16:45.131458Z",
     "iopub.status.idle": "2023-08-27T08:21:02.365844Z",
     "shell.execute_reply": "2023-08-27T08:21:02.364406Z"
    },
    "papermill": {
     "duration": 257.283316,
     "end_time": "2023-08-27T08:21:02.392471",
     "exception": false,
     "start_time": "2023-08-27T08:16:45.109155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| self.n_cols: 769\n",
      "ic| self.means.shape: (384,), self.stds.shape: (384,)\n",
      "ic| tflite_keras_model: <__main__.TFLiteModel object at 0x7a86aa2576a0>\n",
      "ic| converter: <tensorflow.lite.python.lite.TFLiteKerasModelConverterV2 object at 0x7a864cfa2410>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33M\t.//model.tflite\n"
     ]
    }
   ],
   "source": [
    "FLAGS.pad_thre = 0.3\n",
    "FLAGS.decode_phrase_type = False\n",
    "to_tflite_model(keras_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 370.122668,
   "end_time": "2023-08-27T08:21:05.452718",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-27T08:14:55.330050",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
